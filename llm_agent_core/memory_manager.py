import chromadb
import numpy as np
import json
from typing import List, Dict

import re
import os
from typing import Dict, Any


# the semantica segementation  "../scence_data/seg_map/"
def _parse_filename_to_ned(filename: str) -> Dict[str, float]:
    """ä»æ–‡ä»¶å 'X=...Y=...Z=...' ä¸­è§£æå‡º NED åæ ‡ã€‚"""
    match = re.search(r'X=([-+]?\d*\.?\d+)Y=([-+]?\d*\.?\d+)Z=([-+]?\d*\.?\d+)', filename)
    if match:
        return {
            "x": float(match.group(1)),
            "y": float(match.group(2)),
            "z_down": float(match.group(3))
        }
    return None

def _create_semantic_description(data: Dict[str, str], ned_coords: Dict[str, float]) -> str:
    """å°†è¯­ä¹‰ä¿¡æ¯å’Œä½ç½®ç»„åˆæˆä¸€ä¸ªè‡ªç„¶è¯­è¨€æè¿°ã€‚"""
    
    # æ„å»ºè¯­ä¹‰æè¿°
    description = f"ä¸€ä¸ªç±»å‹ä¸º '{data['type']}' çš„ç‰©ä½“/åŒºåŸŸè¢«æ£€æµ‹åˆ°ã€‚ç‰¹å¾æè¿°ï¼š"
    features = [f"{k}: {v}" for k, v in data.items() if k not in ['type', 'filename'] and v != 'test']
    description += ", ".join(features)
    
    # æ·»åŠ ä½ç½®ä¿¡æ¯
    description += (f" ä½äºæ— äººæœºæ‹æ‘„ç‚¹ (NED åæ ‡: X={ned_coords['x']:.2f}, Y={ned_coords['y']:.2f}, "
                    f"æµ·æ‹”é«˜åº¦: {-ned_coords['z_down']:.2f} ç±³)ã€‚")
    
    return description



# --- æ ¸å¿ƒè®°å¿†ç®¡ç†ç±» ---
class MemoryManager:
    """
    Agent çš„é•¿æœŸè®°å¿† (LTM) ç®¡ç†å™¨ï¼Œä½¿ç”¨ ChromaDB å®ç° RAG æœºåˆ¶ã€‚
    ç°åœ¨æ”¯æŒåŸºäºåœºæ™¯ ID çš„æŒä¹…åŒ–å­˜å‚¨ã€‚
    """

    SEG_MAP_BASE_DIR = 'scene_data/seg_map'
    CHROMA_DB_BASE_DIR = "./chroma_dbs"
    
    def __init__(self, scene_name: str, load_static_map: bool = False, embedding_dim: int = 384):
        """
        åˆå§‹åŒ– MemoryManagerã€‚
        
        å‚æ•°:
            scene_name (str): å½“å‰åœºæ™¯çš„å”¯ä¸€æ ‡è¯†ç¬¦ã€‚
            load_static_map (bool): æ˜¯å¦åœ¨åˆå§‹åŒ–æ—¶å¯¼å…¥é™æ€åœ°å›¾æ•°æ®ã€‚
        """
        self.scene_name = scene_name
        self.load_static_map = load_static_map
        self.embedding_dim = embedding_dim
        
        # ğŸŒŸ å…³é”®ä¿®æ­£ 1: å®šä¹‰ collection_name
        self.collection_name = f"uav_memory_{scene_name}" 
        
        # 1. åˆå§‹åŒ– ChromaDB å®¢æˆ·ç«¯ (ä½¿ç”¨æŒä¹…åŒ–æ¨¡å¼)
        db_path = os.path.join(self.CHROMA_DB_BASE_DIR, scene_name)
        os.makedirs(db_path, exist_ok=True) 
        self.client = chromadb.PersistentClient(path=db_path)
        
        # ğŸŒŸ å…³é”®ä¿®æ­£ 2: ä½¿ç”¨ collection_name åˆ›å»ºé›†åˆ
        # æ³¨æ„ï¼šåœ¨çœŸå®çš„ ChromaDB é›†æˆä¸­ï¼Œname å‚æ•°æ˜¯ get_or_create_collection å¿…éœ€çš„
        self.collection = self.client.get_or_create_collection(
            name=self.collection_name  # <--- ä½¿ç”¨å®šä¹‰çš„åç§°
            # embedding_function=... # çœŸå®é¡¹ç›®éœ€æŒ‡å®š
        )
        
        print(f"è®°å¿†ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸï¼Œåœºæ™¯: {self.scene_name}ï¼Œé›†åˆ: {self.collection_name}")
        print(f"é™æ€åœ°å›¾åŠ è½½çŠ¶æ€: {'å¯ç”¨' if self.load_static_map else 'ç¦ç”¨'}ã€‚") 

    
    def check_and_initialize_scene_data(self) -> str:
        """
        æ£€æŸ¥åœºæ™¯é™æ€æ•°æ®æ˜¯å¦å·²å¯¼å…¥ã€‚ç°åœ¨å— load_static_map æ ‡å¿—æ§åˆ¶ã€‚
        """
        
        # å¦‚æœ load_static_map ä¸º Falseï¼Œåˆ™ç›´æ¥è·³è¿‡å¯¼å…¥
        if not self.load_static_map:
            return f"OBSERVATION: é™æ€åœ°å›¾æ•°æ®åŠ è½½å·²ç¦ç”¨ (load_static_map=False)ã€‚"

        # å¦‚æœ load_static_map ä¸º Trueï¼Œåˆ™ç»§ç»­æ£€æŸ¥é›†åˆæ˜¯å¦æœ‰æ•°æ®
        
        jsonl_filename = f"{self.scene_name}_seg_map.jsonl" # å‡è®¾æ–‡ä»¶å‘½åçº¦å®š
        jsonl_filepath = os.path.join(self.SEG_MAP_BASE_DIR, jsonl_filename)
        
        if self.collection.count() > 0:
            # æ³¨æ„ï¼šå³ä½¿ load_static_map=Trueï¼Œå¦‚æœæ•°æ®åº“ä¸­å·²æœ‰æ•°æ®ï¼Œæˆ‘ä»¬ä»ç„¶è·³è¿‡å¯¼å…¥ï¼Œä½¿ç”¨å·²æœ‰çš„æŒä¹…åŒ–æ•°æ®
            print(f"âœ… åœºæ™¯ '{self.scene_name}' è®°å¿†åº“å·²åŒ…å« {self.collection.count()} æ¡è®°å½•ï¼Œè·³è¿‡é™æ€å¯¼å…¥ã€‚")
            return f"OBSERVATION: åœºæ™¯ '{self.scene_name}' è®°å¿†åº“å·²åˆå§‹åŒ–ï¼ˆåŒ…å«é™æ€æ•°æ®ï¼‰ã€‚"
        else:
            print(f"âš  åœºæ™¯ '{self.scene_name}' è®°å¿†åº“ä¸ºç©ºï¼Œload_static_map=Trueï¼Œæ­£åœ¨å°è¯•ä» {jsonl_filepath} å¯¼å…¥...")
            # å¯¼å…¥é™æ€æ•°æ®
            return self.import_semantic_jsonl_data(jsonl_filepath)
            

    def _get_embedding(self, text: str) -> List[float]:
        """
        ã€æ¨¡æ‹Ÿå‡½æ•°ã€‘å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡åµŒå…¥ã€‚
        åœ¨çœŸå®é¡¹ç›®ä¸­ï¼Œè¿™é‡Œåº”è°ƒç”¨ OpenAI, Cohere, æˆ– HuggingFace ç­‰åµŒå…¥æ¨¡å‹ APIã€‚
        """
        # PoC æ¨¡æ‹Ÿï¼šè¿”å›ä¸€ä¸ªå½’ä¸€åŒ–çš„éšæœºå‘é‡
        vector = np.random.rand(self.embedding_dim)
        return (vector / np.linalg.norm(vector)).tolist()

    # --- LLM-Agent å·¥å…· 1: å­˜å‚¨/æ›´æ–°è®°å¿† ---
    
    def update_search_map(self, coordinates: Dict[str, float], status: str, description: str) -> str:
        """
        ã€LLM-Agent å·¥å…·ã€‘Agent å‘ç°æ–°çº¿ç´¢æˆ–å®Œæˆæœç´¢åŒºåŸŸæ—¶è°ƒç”¨ã€‚
        
        å‚æ•°:
            coordinates (dict): å‘ç°ç‚¹çš„ GPS åæ ‡ (latitude, longitude, altitude)ã€‚
            status (str): è®°å¿†çš„çŠ¶æ€ (ä¾‹å¦‚: "çº¿ç´¢", "å·²æœç´¢", "ç¦é£åŒº")ã€‚
            description (str): è¯¦ç»†æè¿°ã€‚
            
        è¿”å›:
            str: è®°å¿†æ›´æ–°ç»“æœçš„è§‚å¯ŸæŠ¥å‘Šã€‚
        """
        # æ„å»ºç”¨äºåµŒå…¥å’Œå­˜å‚¨çš„æ–‡æœ¬
        memory_text = f"åœ¨åæ ‡ (Lat:{coordinates['latitude']:.5f}, Lon:{coordinates['longitude']:.5f}) å¤„ï¼šçŠ¶æ€='{status}'ï¼Œæè¿°='{description}'ã€‚"
        
        embedding = self._get_embedding(memory_text)
        
        # ä¸ºæ¯ä¸ªç‚¹ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„ ID
        item_id = str(len(self.collection.get()['ids']) + 1)
        
        self.collection.add(
            embeddings=[embedding],
            documents=[memory_text],
            metadatas=[{"status": status, "coordinates": json.dumps(coordinates), "description": description}],
            ids=[item_id]
        )
        
        return f"OBSERVATION: é•¿æœŸè®°å¿†æ›´æ–°æˆåŠŸã€‚æ–°å¢è®°å¿† ID: {item_id}ï¼Œå†…å®¹ï¼š'{memory_text}'ã€‚"

    # --- LLM-Agent å·¥å…· 2: å¬å›å†å²çº¿ç´¢ (RAG) ---

    def retrieve_historical_clues(self, query: str, top_k: int = 3) -> str:
        """
        ã€LLM-Agent å·¥å…·ã€‘æ ¹æ® Agent çš„è¯­ä¹‰æŸ¥è¯¢ï¼Œå¬å›æœ€ç›¸å…³çš„å†å²çº¿ç´¢ã€‚
        
        å‚æ•°:
            query (str): LLM Agent æå‡ºçš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢ (ä¾‹å¦‚: "æˆ‘æœ€ååœ¨å“ªé‡Œçœ‹åˆ°äº†çº¢è‰²çš„ä¸œè¥¿?")ã€‚
            top_k (int): å¬å›æœ€ç›¸å…³çš„è®°å¿†æ•°é‡ã€‚
            
        è¿”å›:
            str: å¬å›çš„å†å²çº¿ç´¢ï¼Œä»¥ç»“æ„åŒ–æ–‡æœ¬å½¢å¼è¿”å›ç»™ LLMã€‚
        """
        
        query_vector = self._get_embedding(query)
        
        # ä½¿ç”¨ ChromaDB è¿›è¡Œç›¸ä¼¼æ€§æœç´¢
        results = self.collection.query(
            query_embeddings=[query_vector],
            n_results=top_k,
            include=['documents', 'metadatas', 'distances']
        )
        
        clues_list = []
        if results and results['documents'] and results['documents'][0]:
            
            for doc, meta, dist in zip(results['documents'][0], results['metadatas'][0], results['distances'][0]):
                clues_list.append(
                    f"[è·ç¦» {dist:.4f}]: {doc}"
                )
        
        if not clues_list:
            return "OBSERVATION: é•¿æœŸè®°å¿†ä¸­æ²¡æœ‰å‘ç°ä¸æŸ¥è¯¢ç›¸å…³çš„å†å²çº¿ç´¢ã€‚"
        
        # å°†å¬å›ç»“æœæ ¼å¼åŒ–ï¼Œä»¥ä¾¿ LLM å®¹æ˜“è§£æå’Œæ•´åˆåˆ° Thought ä¸­
        return "OBSERVATION: å¬å›çš„å†å²çº¿ç´¢å¦‚ä¸‹ï¼š\n" + "\n".join(clues_list)
    

    def import_semantic_jsonl_data(self, jsonl_filepath: str) -> List[str]:
        """
        ä» JSONL æ–‡ä»¶ä¸­å¯¼å…¥è¯­ä¹‰åˆ†å‰²æ•°æ®åˆ° RAG å‘é‡æ•°æ®åº“ã€‚
        """
        imported_ids = []
        documents_to_add = []
        metadatas_to_add = []
        
        try:
            with open(jsonl_filepath, 'r') as f:
                for line in f:
                    try:
                        data = json.loads(line.strip())
                        
                        # 1. è§£æåæ ‡
                        ned_coords = _parse_filename_to_ned(data['filename'])
                        if not ned_coords:
                            continue
                        
                        # 2. åˆ›å»ºè¯­ä¹‰æ–‡æ¡£å’Œå…ƒæ•°æ®
                        semantic_doc = _create_semantic_description(data, ned_coords)
                        
                        # 3. æ„é€ å…ƒæ•°æ® (å°†æ‰€æœ‰åŸå§‹å­—æ®µä¹Ÿå­˜å…¥ï¼Œä¾¿äºæ£€ç´¢)
                        metadata = {**data, **ned_coords, 'source': 'semantic_segmentation', 'status': 'detected'}
                        
                        # 4. å­˜å‚¨
                        documents_to_add.append(semantic_doc)
                        metadatas_to_add.append(metadata)
                        # ä½¿ç”¨ UUID æˆ–ä¸€ä¸ªåŸºäºå†…å®¹çš„å“ˆå¸Œå€¼ä½œä¸º ID
                        doc_id = f"sem_{hash(semantic_doc)}".replace('-', '')
                        imported_ids.append(doc_id)
                        
                    except json.JSONDecodeError:
                        print(f"è­¦å‘Š: è·³è¿‡æ— æ•ˆçš„ JSONL è¡Œ: {line.strip()[:50]}...")
                        continue

            if documents_to_add:
                # å‡è®¾æ‚¨ä½¿ç”¨ ChromaDBï¼Œå¹¶æœ‰ä¸€ä¸ª self.collection å¯¹è±¡
                # self.collection.add(
                #     documents=documents_to_add,
                #     metadatas=metadatas_to_add,
                #     ids=imported_ids
                # )
                print(f"âœ… æˆåŠŸå¯¼å…¥ {len(documents_to_add)} æ¡è¯­ä¹‰åˆ†å‰²è®°å½•åˆ°è®°å¿†åº“ã€‚")
                return imported_ids
            
            return []

        except FileNotFoundError:
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {jsonl_filepath}")
            return []

# --- ç¤ºä¾‹ç”¨æ³• ---

if __name__ == "__main__":
    memory_manager = MemoryManager()

    # 1. æµ‹è¯•è®°å¿†å­˜å‚¨
    test_coords_1 = {"latitude": 47.641, "longitude": -122.140, "altitude": 30.0}
    test_coords_2 = {"latitude": 47.642, "longitude": -122.139, "altitude": 35.0}

    # è®°å¿† 1: å‘ç°çº¿ç´¢
    result_1 = memory_manager.update_search_map(
        test_coords_1, "çº¿ç´¢", "å‘ç°ä¸€ä¸ªçº¢è‰²çš„èƒŒåŒ…ï¼Œä½†æ— äººæœºæ— æ³•é è¿‘ã€‚"
    )
    print(result_1)

    # è®°å¿† 2: æ¢ç´¢å®Œæˆ
    result_2 = memory_manager.update_search_map(
        test_coords_2, "å·²æœç´¢", "å®Œæˆäº†è¯¥åŒºåŸŸçš„ç½‘æ ¼æœç´¢ï¼Œæ²¡æœ‰å…¶ä»–å‘ç°ã€‚"
    )
    print(result_2)

    # 2. æµ‹è¯• RAG å¬å›
    print("\n--- Agent è¯¢é—®ï¼šçº¢è‰²çš„çº¿ç´¢åœ¨å“ªé‡Œï¼Ÿ ---")
    retrieval_query_1 = "çº¢è‰²çš„çº¿ç´¢åœ¨å“ªé‡Œï¼Ÿ"
    retrieval_result_1 = memory_manager.retrieve_historical_clues(retrieval_query_1)
    print(retrieval_result_1)
    
    print("\n--- Agent è¯¢é—®ï¼šæˆ‘æœå®Œäº†å“ªäº›åœ°æ–¹ï¼Ÿ ---")
    retrieval_query_2 = "å“ªäº›åœ°æ–¹å·²ç»å®Œæˆäº†æœç´¢ï¼Ÿ"
    retrieval_result_2 = memory_manager.retrieve_historical_clues(retrieval_query_2)
    print(retrieval_result_2)